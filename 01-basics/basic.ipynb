{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic autograd example 1\n",
    "\n",
    "Autograd adalah salah satu fitur inti dari PyTorch yang membuatnya menjadi framework yang sangat populer untuk penelitian dan pengembangan dalam bidang deep learning. \n",
    "\n",
    "Autograd adalah mesin diferensiasi otomatis yang memungkinkan PyTorch untuk menghitung gradien secara efisien dari fungsi yang kompleks, seperti yang umum ditemukan dalam jaringan saraf tiruan.\n",
    "\n",
    "Diferensiasi mengacu pada proses menghitung gradien dari suatu fungsi. Gradien ini kemudian digunakan dalam algoritma optimasi seperti gradient descent untuk memperbarui parameter model secara iteratif sehingga model dapat belajar dari data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara Kerja Autograd?\n",
    "\n",
    "* Tensor: Semua tensor dalam PyTorch memiliki atribut requires_grad yang secara default bernilai False. Jika diubah menjadi True, PyTorch akan mulai melacak semua operasi yang dilakukan pada tensor tersebut.\n",
    "* Computational Graph: Ketika Anda melakukan operasi pada tensor dengan requires_grad=True, PyTorch secara otomatis membangun sebuah computational graph. Graph ini merepresentasikan hubungan antara operasi-operasi yang dilakukan pada tensor tersebut.\n",
    "* Backward Pass: Setelah melakukan forward pass (menghitung output dari model), Anda dapat memanggil metode .backward() pada tensor output untuk menghitung gradien dari semua tensor yang terlibat dalam computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7273,  0.5686,  0.7155], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# buat tensor dengan requires_grad= True \n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * 2\n",
    "z = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() # for  requires_grad=False RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6667, 0.6667, 0.6667])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors\n",
    "\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a computational graph\n",
    "\n",
    "y = w*x*b\n",
    "\n",
    "# compute gradients\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n",
      "tensor(3.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# print out the gradients\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic autograd example 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.0950,  0.3328, -0.1448],\n",
      "        [-0.4615,  0.2062, -0.3463]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([0.2422, 0.5299], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create tensors of shape (10, 3) and (10, 2)\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# build a fully connected layer \n",
    "linear = nn.Linear(3, 2)\n",
    "print('w: ', linear.weight)\n",
    "print('b:', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass (menghitung output model)\n",
    "\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2291069030761719\n"
     ]
    }
   ],
   "source": [
    "# Compute loss \n",
    "loss = criterion(pred, y)\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[ 0.3929,  0.5996,  0.3243],\n",
      "        [-0.1792,  0.1416,  0.0720]])\n",
      "dL/db: tensor([0.5514, 0.2776])\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients\n",
    "print ('dL/dw: ', linear.weight.grad)\n",
    "print('dL/db:', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization: 1.21860671043396\n"
     ]
    }
   ],
   "source": [
    "# you can also perform gradient descent at the low level \n",
    "# linear.weight.data.sub(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# print out the loss after 1-step gradient descent\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading data from numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# create a numpy array\n",
    "x = np.array([[1, 2], [3,4]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# convert the numpy array to a torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# convert the torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# download and construct CIFAR-10 dataset\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# fetch one data pair (read data from disk)\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader (this provides queueu and threads in a very simple way)\n",
    "train_loader = torch.utils.data.DataLoader(dataset= train_dataset, batch_size=64, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when iteration starts, queue and thread start to load data from files\n",
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# mini batch images and labels\n",
    "for images, labels in data_iter:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Input pipeline for custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image  # Assuming PIL for image processing\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image  # Assuming PIL for image processing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the custom dataset.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing images and labels.\n",
    "            transform (torchvision.transforms, optional): Transformations to apply to images. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Process data directory structure (modify based on your data organization)\n",
    "        for class_name in os.listdir(data_dir):\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for filename in os.listdir(class_path):\n",
    "                    image_path = os.path.join(class_path, filename)\n",
    "                    if os.path.isfile(image_path):  # Ensure it's a valid image file\n",
    "                        self.image_paths.append(image_path)\n",
    "                        self.labels.append(class_name)  # Assuming class name is the label\n",
    "\n",
    "        # Check if there are any images loaded\n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(\"No images found in the data directory.\")\n",
    "\n",
    "        self.transform = transform  # Optional transformation for data augmentation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Gets one data pair (image and label) from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the data item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the processed image (tensor) and its corresponding label (tensor).\n",
    "        \"\"\"\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Load image (modify based on your image format)\n",
    "        image = Image.open(image_path).convert('RGB')  # Assuming RGB images\n",
    "\n",
    "        # Apply transformations (if provided)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to a tensor (modify based on label format)\n",
    "        label_tensor = torch.tensor(label)  # Assuming class names as labels\n",
    "\n",
    "        return image, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of data samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Create the custom dataset with data directory\n",
    "custom_dataset = CustomDataset(data_dir=\"./data\")  # Replace with your actual data directory\n",
    "\n",
    "# Create the data loader\n",
    "train_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\immab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\immab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Download and load the pretrained ResNet-18\n",
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to finetune only the top layer of the model, set as below\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# replace the top layer for finetuning\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)\n",
    "\n",
    "# Forward pass\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\immab\\AppData\\Local\\Temp\\ipykernel_496\\1556255639.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('model.ckpt')\n"
     ]
    }
   ],
   "source": [
    "# save and load the entire model\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\immab\\AppData\\Local\\Temp\\ipykernel_496\\250726142.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet.load_state_dict(torch.load('params.ckpt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
